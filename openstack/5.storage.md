# 延伸阅读1

目前比较主流的分布式存储（包括Ceph）主打的宣传亮点基本上包含了：<br>
- 算力+性能加速（AI等），确保单节点的性能（比如16.8万IOPS@1ms稳定时延）
- 存储服务合一（比如： 块，文件，对象，HDFS四合一）
- 云协同，全生命周期管理等


## 云和AI时代，数据迎来海量增长
数据增长量，2018年全球32.5ZB, GIV展望，2025年间达到180ZB：
- 5G带来可预期的数据10倍增长
- 8K视频10TB/小时，相比1080P增长了40倍
- 自动驾驶每车每天产生的数据量是60TB
- 某银行大数据精准营销，用户日志年增量PB级

## 企业不断扩展业务边界，分布式架构成为首选

业务需求来源不断扩大：
- 云化整合：<br>
云场景存储资源整合，快速应对不可预知的业务变化
- 海量扩展：<br>
非结构化数据快速增长，需要更高可靠性、更高性能的存储
- 更低的TCO：<br>
高效冗余和数据缩减算法，降低海量数据存储TCO

存储空间变化趋势明显：
从2015年到2021年，分布式，全闪存份额增长趋势明显；

1. 在过去，企业业务单一，少量的存储设备、单一的存储类型即可满足需求
2. 随着企业不断拓展业务边界，有越来越多的云化整合、海量扩展和降TCO的需求，分布式存储成为首选
3. 根据IDC和XXMI的分析显示，存储市场空间正发生着变化，分布式和全闪存将成为主流


## 运营商：建设统一平台，加速多域融合，实现降本增效

统一的平台来支撑三域应用及B2B应用：
- B域应用：CRM，BOSS，经验分析，精准营销；
- M域应用：ERP，财务，人力，资产管理等；
- O域应用：资源管理，业务编排，安全管控，资产管理
- B2B：计算服务，存储服务，网络服务，灾备；

来自中移动的关键需求：
- 弹性扩容<br>
单套存储百PB、按需灵活扩容
- 高性能<br>
百万IOPS，性能随容量线性增长
- 高可用<br>
支撑关键业务7x24h不间断运行
- 低TCO<br>
统一运维管理，无需数据迁移

## 金融：核心系统闪存化，外围业务全面分布式化
来自招行的关键需求
- 弹性部署：按需小时级部署，快速应对突发业务浪涌，降低新业务TTM
- 大集群：单套存储百PB、承载多个业务系统
- 海量扩展：千亿级小对象存储能力，应对电子票据需求
- 低TCO：大数据存算分离，通过解耦实现大数据云化、打通多平台数据孤岛

金融行业常见的业务架构

前端： 渠道接入系统  <->  前置系统 <->  产品服务（核心系统，生产管理系统，CRM系统） <-> 电子影像平台

平台层：数据集成平台（交换平台） <-> 大数据分析平台

内部： 开发测试，办公OA等

## 各行各业以分布式存储为基座推进云战略，加速数字化转型

1. 运营商<br>
5G带来10倍连接及业务多样化预期，持续推进IT系统集中化和业务云化
- 日志留存和竞分大数据
- B2B/B2C内容存储
- BOM域云化
2. 安平<br>
通过大数据、AI实现跨地域、多警种的数据融合和业务协同，提升预测、预警、预防能力
- 离线分析/实时检索大数据
- 非结构化原始库/归档
- 警务云平台
3. 金融<br>
移动互联业务量超过传统业务，精准营销实现千人千面，云、大数据带来数据的海量增长
- 渠道接入
- 票据影像/双录
- 离线大数据分析
4. 大企业<br>
工业4.0，无人驾驶、车联网、4K/8K高清等新产业驱动数据海量增长
- 制造/HPC/媒资/科研文件存储
- 行业云

## 企业云化转型最佳数据底座 - 分布式存储
极致能力：
- 极速体验：<br>
首个支持企业关键应用的分布式存储 （最高性能分布式存储450万IOPS@0.787ms, 企业应用环境中验证，满足易用性、性能和可靠性要求）
- 极致效率：<br>
领先的数据冗余和缩减算法大幅节省TCO
- 智能管理：<br>
基于AI的全生命周期智能管理提升管理效率

## 专用硬件

1. 更高性能， 同等功耗，性能相比x86高15%+
- 绑核机制，提升20%
- 多种算法卸载至ARM，性能提升10%
- FlashLink 0.5ms超低时延
2. 更高可靠性， 完整的硬件故障检测和隔离
- DIF据完整性保护
- 全面亚健康检测：网络/存储/计算/进程
- 精细化IO性能统计
3. 更好兼容性， 一体化部署和交付
- 更清晰故障定界
- 器件质量保障，严格的来料控制和测试

## 商业模式（可用容量，所买即所得）
传统商业模式，硬件统一，软件按照裸容量采购；这种模式下，客户明显吃亏和不平衡： 各个厂商软件能力参差不齐，硬件配置一致，但是客户实际使用的可得容量差距很大；

可用容量（或者可得容量）模式，以实际业务需求为出发点进行采购。有点很明显：
- 相同数据需求下，更少设备，更低采购和维护成本；
- 统一语言，业务，IT和采购部门统一语言，业务需求直接作为采购需求
业务部门： 我需要XPB的可得容量
IT部门：我需要XPB的可得容量
采购部门： 我需要XPB的可得容量
-> 各个厂商参考的招标依据一样，竞争力差异就体现出来了；

## 性能与高端存储媲美
XXEC特点：
1. 磁盘利用率更高：块EC利用率可高达80%；对象、文件EC磁盘空间利用率可高达90%；（VSAN最高只有75%理论）
2. 配置更灵活：N+M冗余比灵活配置，客户可根据不同的业务诉求配置不同的冗余策略。（块：3+1，3+2，4+2，8+2，12+3；文件对象更多，详见冗余配比表。VSAN只支持3+1和4+2）
3. 性能表现更优越：基于小IO场景下SSD Cache做EC条件缓存技术，块存储开启EC场景下，时延表现优于VSAN 40%。
4. 场景更广：支持HDD与SSD介质场景。（VSAN仅能在SSD场景下使用。）

450万IOPS@0.787ms稳定时延, 交易型数据库大压力时性能无波动

## 智能CPU分区算法，时延缩短20%

1. CPU分组算法<br>
IO分组避免相互干扰

- 专核专用，保障关键业务资源投入，降低时延
- 多核共用，自适应调配多个业务的负载均衡

2. CPU分核算法<br>
IO连续执行避免切换消耗

一次请求在同一个核上连续执行，利用同核不发生线程切换从而保证操作原子性来实现免锁设计，避免频繁多核切换，提升CPU Cache命中率

3. IO智能调度
I/O优先级保障，降低业务影响

数据读写I/O永远第一优先级响应，其他I/O等数据读写I/O完成后再重新启动，保障数据读写I/O时延最低

综合上述算法：可达0.5ms 稳定低时延，关键业务极速响应

详情解读

数据从I/O卡流入CPU。XX分布式存储的CPU利用算法具备三大显著优势：
1. CPU采用最新的智能分区算法，包括CPU分组算法和CPU分核算法。CPU分组算法是把比较重要的I/O单独一个分组（比如：读写，数据交换），从而读写I/O与其他I/O部署在不同分组，避免相互干扰，保障读写I/O的性能；优先级相对低的I/O在一个分组内，共享CPU资源。这样能够保障资源利用的最大化。
2. .CPU分核算法是XX独有的优势。我们知道，I/O工作在CPU的时间片是轮询的，一段时间在CPU的Core1上工作，过一段时间轮询到Core2上工作，其一，从Core1跳转到Core2是要花时间的。
其二，为了保障数据的一致性，I/O从core1跳转到core2时，CPU core1会加锁，保证I/O不会在Core1上运行，只在core2上运行，给I/O加锁也是需要时间的。当I/O在core2 上工作一段时间，如果他要跳转回Core1，他需要为Core2加锁，同时为Core1解锁，此时加锁和解锁都是需要时间的。

CPU分核算法，能很好解决CPU轮询和加/解锁的问题，它让1次读请求或者写请求就在CPU的1个core上连续执行，更为重要的是，它采用了免锁设计，这样一个I/O就在一个core上运行，从开始到结束，有效避免了多个Core之间切换，以及加锁、解锁之间的时间消耗。

数据从CPU进入缓存。

缓存对存储而言是非常重要的，缓存算法的优劣是决定存储性能的关键要素之一，缓存算法具备三大亮点：
1. 摒弃传统存储的二叉树算法，采用闪存专用的哈希表算法。众所周知，二叉树算法的空间占用少，但是查找速度慢；哈希表算法查找速度快，空间占用高，采用多级哈希即满足空间占用同时满足高效查找速度
2. 元数据和数据缓存资源分区提升元元数据命中率，读数据和写数据缓存分区避免读写缓存资源互抢；
3. 哈希索引支持冷热分区机制，提升查找速度

## 智能Cache优化算法提升Cache效率，时延波动率低于2%

1. 智能缓存分区<br>
三个区： 元数据区， 写数据区， 热点读数据区
- 元数据长期驻留内存，采用智能淘汰算法优先淘汰冷数据，提升缓存元数据命中率
- 独立缓存空间存放顺序读I/O，空间大小基于业务模型智能调整
- 智能预取算法提前获取热点数据，缩减访问时延

2. 智能分区索引表<br>
分热表，温表，冷表；
- 根据数据访问的冷热频率分成3张分区索引表，提升数据查询速度

## 智能识别数据类型和IO大小，提升效率
大IO和小IO做智能识别。
1. 智能IO聚合与满分条写<br>
- 大IO直接组EC分条下盘，不过Cache，节省Cache资源并提高缓存寿命
- 小IO基于WAL追加写技术，聚合成满分条EC下盘，降低写放大
- 将不同LUN的随机小IO写聚合成100%顺序写，提升性能

2. 元数据和数据独立分区存储<br>
元数据和数据分区存放，元数据采用LSM Tree的Merge方式，数据通过Plog的GC方式，实现垃圾空间回收，降低系统写放大

## 支持NVMe全闪存架构，提供更高品质的服务
NVMe架构缩短端到端时延 45%
- CPU和SSD直接通信，缩短传输路径（SAS全闪存，该段时延可达562微秒， NVMe全闪存该段时延可达314微秒）
- 并发数提升10倍，性能更优
- 协议交互从4次减少为2次，写请求处理效率提升1倍

1. 率先在分布式存储支持NVMe架构：领先于VSAN、CEPH
2. 支持独立NVMe形态，同时支持NVMe和SAS/SATA混插形态：助力用户向NVMe全闪存时代平滑演进

## 垮集群AA双活，支持关键业务数据库的高可用
可以把传统高端存储领域的HyperMetro双活能力，平滑移植到了分布式存储中，实现跨地域的A-A双活，支撑企业关键业务7*24小时稳定运行。
1. 利用独有Fastwrite技术，在同等组网下，时延降低50%
2. “RDMA+”算法优化，实现IP网络百公里业界最低时延

特点：
1. 大规模：全分布式A-A 双活
2. 仲裁模式：第三方仲裁服务器，静态优先级

## 弹性EC，提升磁盘利用率

原始存储一份，三分副本，同时提供弹性EC（包含校验和数据）的方案，支持
1. 全闪或者混合
2. 支持最大配比22+2，或者20+4
3. 最大可容忍4节点故障；

同等硬件配置，有效磁盘使用率可达91%， EC性能相同可靠性副本不降

支持副本和EC冗余机制：
- 机柜级冗余：最大容忍4个机柜同时失效
- 节点级冗余：最大容忍4个节点同时失效
- 硬盘及冗余：最大容忍4个硬盘同时失效

分布式存储提供多副本和纠删码（EC, Erasure code）两种数据冗余保护方式。
多副本方式：支持2副本或3副本，保障硬盘级、节点级、机柜级冗余，最大可支持2机柜同时故障情况下业务不中断。
EC方式：支持3+1，2+2，3+2，3+2(:1)，4+2，4+2(:1) ，8+2，8+2(:1) ，12+3，12+3(:1) 等9种冗余配比，最大可支持3节点同时失效情况下业务不中断。
交底：当+1或+2冗余时，冗余效果不受就否打开EC cache影响；当+3冗余且打开EC cache时，最大容忍2节点或3机柜同时失效；当+3冗余且关闭EC cache时，最大容忍3节点或3机柜同时失效。

## 设备级可靠: 最全面的亚健康智能检测与预处理，提前排除故障风险

亚健康检查和处理

亚健康： 指系统资源性能降级的情况，可能导致业务系统变慢甚至终端

主要涉及：
- 硬盘， 包含故障，慢盘，坏块，Smart信息，UNC错误等
- SSD卡， 包含故障，慢卡， 坏块，高温故障，电容失效，读写次数超过亚健康阈值
- 服务器，包含CPU资源耗尽，CPU频率降低等
- 网络，包含网卡失效，降速，链路丢包，降速，端口故障，闪断，丢包等

详情：

分布式存储XX支持硬盘Smart检测、快慢盘检测、磁盘SCSI错误处理、硬盘热插拔和识别处理、磁盘扫描等，上层业务根据Smart Data返回的相关IO错误和磁盘状态信息， 完成读修复、磁盘移除和重建、坏块标记、有效数据磁盘扫描、Smart超阈值和慢盘处理（预重建后移除磁盘）。

读修复功能（Read  Repair）

Read Repair是一种在读操作时，当发现有读失败，会判断错误类型，如果发现是磁盘扇区读取错误，可以通过从其它副本读取数据，然后重新写入的方法进行恢复。这是磁盘的特性，对大部分读扇区错误可以修复。如果此方法还不能修复，那么就通过隔离流程为副本选择其它硬盘并把故障的硬盘踢出集群。

硬盘：UNC错误是“校验错误”的意思
SSD卡：11000≤磨损度认为处于亚健康状态
服务器：CPU资源耗尽或频率降低，体现在CPU变慢，内存故障，最终体现为节点因服务器部件故障而变慢。
网络：网络丢包、错包，亚健康阈值为4%。1分钟内4次超过阈值则认为处于亚健康状态。网口故障立即出发亚健康。端口闪断1分钟6次、连续3分钟有闪断，则认为处于亚健康。网卡降速立即出发亚健康。链路丢包：60s内30个包丢失2个则认为亚健康。

## IO级可靠：端到端DIF一致性校验，保障数据完整性

1. 数据写入前： 插入校验位,
2. 数据落盘时： 检查校验位
3. 数据读出时： 检查校验位

三个大的流程
1. 在线校验<br>
主机侧写入校验位，数据写盘和和主机读出时进行双校验
2. 后台校验<br>
系统负载较低时，后台自动进行周期性校验
3. 无感自愈<br>
损坏数据本地冗余数据修复，或通过双活数据修复


静默错误: Silent Data Corruption, SDC.
此处“0.00000001% ”隐含了2018年7月腾讯云SDC问题导致用户数据丢失的案例，腾讯云宣称其云硬盘服务可提供99.9999999%（9个9）数据可靠性。
而“100%的投入”是说：虽然静默错误发生的概率很低、而DIF功能开发投入巨大，以致于各分布式存储厂商不愿意投入开发DIF功能解决此问题，XX将分布式存储产品性做到极致，提供端到端DIF较验功能，有效避免静默错误问题。

静默数据错误：数据在磁盘的写入、读出、保存过程中，要经过多个部件、多种传输通道和复杂的软件处理，如果数据被破坏，可能会导致数据错误。若错误无法被立即检测出来，而是在后续应用中访问所保存的数据时才发现，叫做静默数据错误。
XX分布式存储 提供端到端DIF（DIF：Data Integrity Field）校验功能，数据写入磁盘前生成校验值，数据从磁盘读取时自动进行CRC校验，校验失败，尝试通过副本修复并上报告警。当发起的静默错误达到一定阈值时，系统自动隔离异常磁盘。

腾讯云运维关闭数据检验功能，导致其用户“前沿数控”数据丢失事件
“前沿数控”是一家创业公司，定位于数控、模具、机械行业，后转型为一站式平台，开发网站、H5、小程序产品。购买了腾讯云服务。
腾讯云“前沿数控”数据丢失事件始末：2018年7月20日由于TX运维人员在进行日常数据“搬迁扩容”操作前，手动关闭“数据校验”功能，导致“前沿数控”网站、小程序、H5无法登陆云服务器，腾讯云答复云硬盘故障正在紧急恢复；7月22日腾讯云官方承认数据彻底丢失无法修复，并于8月7日发表《关于客户前沿数控数据完整性受损的技术复盘》，指出是由于单副本数据错误+数据迁移前关闭校验+立即删除迁移前的数据导致客户数据丢失导致，数据校验的关闭是此次事故的最关键因素。可见，数据检验在专业数据存储系统中的重要性。

## 10秒节点故障快速切换，业务感知最小化
配合智能网卡等部件，可进一步将故障检测通知时间缩短到毫秒级，提升整体切换效率1倍【2020年】

包含：
1. 进程故障检测
2. 快速心跳转发
3. 快速业务接管
4. 快速IO重定向

XX分布式存储使用普通网卡的通用服务器时，切换时间为10秒，H1版本；使用XX定制智能网卡的服务器时，切换时间为5秒，H2版本。
2019年H2时，CEPH XSKY和VSAN的切换能力为15秒-30秒。

故障快速切换技术原理：

进程故障检测通知，部署监控进程毫秒级监控进程故障后立即拉起进程并通知控制主，控制主判定为进程故障则触发业务本地恢复。
心跳检测机制，针对网络/节点等故障场景导致控制主和业务进程同时失联时，采用“心跳群发+关联诊断”算法在控制新主上快速判定业务进程失联/故障。

快速倒换接管，将需要接管的业务并发倒换到剩余的多节点上，一方面缩短接管时长，一方面使接管后性能更均衡。同时倒换流程使用延迟加载、并行加载、加载数据量缩减、索引快速重建等技术来加速接管过程。

主机IO重定向，VBS层毫秒级检测发往某节点/进程的IO是否返回，如无返回则主动拉取该节点/进程状态视图，如状态视图标示该节点/进程故障则主动断开与该节点网络连接并将IO重定向到新节点/进程。

##  QoS优化：混合负载性能激增场景，实现关键业务性能的智能服务

1. 灵活的QOS调速策略：按盘限制IOPS 带宽；按容量限制IOPS 带宽 ；
2. 灵活的QOS配置策略：可以按照存储池配置QOS策略，也可以按照单个盘设置；
20000是每个卷的最大值？
3000是限死的吗？是否可以针对不同的卷设不同的burst值？
持续时间可调吗

规格及约束：
- QoS策略最大支持64万个；
- 卷的QoS策略高于池的；
- 不支持卷组/共享卷；
- 一个卷或池不能关联多个QoS；
- 删除QoS前，需要先解关联卷或池；
- 卷迁移后，QoS策略不会继承；
- 快照不会继承原卷的QoS；

令牌桶算法：

令牌桶中存放着令牌，只有拿到令牌的IO数据包才能通过，否则会被加入到等待队列中，等待下次调度；

令牌桶分C桶和E桶，C桶大小为Burst的大小，E桶大小为(Burst – 上限) * Burst时长；

使用Burst时，令牌持续从E桶借出到C桶，平时则从C桶存入E桶，直至达到E桶上限。

IO处理流程：

消息IO入对应卷的队列后，Target VBS查找对应卷的上限令牌桶；
根据上限令牌桶查找结果，做如下处理：
- 如果该卷存在上限令牌桶，则从上限令牌桶申请一定数量的令牌，如果成功申请到令牌，则该IO到下一个IO处理模块处理，且令牌桶令牌数减少。
- 如果该卷不存在上限令牌桶（即没有设置QoS），则该IO直接到下一个IO处理模块处理。

对于未成功申请到令牌的IO，会加入一个新的等待队列，在下一个100ms往令牌桶填充令牌数时，优先调度这个等待队列的IO，即，保证IO先到先处理的原则。



## 大数据计算存储分离，弹性EC技术实现TCO节省50%

一体化部署：
1. 顶层包含离线分析，日志留存，经营分析
2. 中间层： 大数据组件
3. 底层： 管理节点，计算存储节点，计算存储节点，计算节点N，组成的Hadoop集群

存算分离部署：
1. 顶层包含离线分析，日志留存，经营分析
2. 中间层： 大数据组件
3. 底层分为两大部分： Hadoop计算集群包含管理节点，计算节点；分布式存储大数据存储集群包含存储节点；通过原生HDFS提供为计算提供存储能力

关键客户价值
1. 计算存储分离<br>
按需独立扩展，避免资源浪费
2. 弹性EC<br>
磁盘利用率从33%提升到91%，单位成本容量提升1.75倍
3. 单目录百亿级文件<br>
全局命名空间，无需拆分目录，简单省事
4. 广泛的兼容<br>
兼容XX FusionInsight、Cloudera、HortonWorks等

## 动态重删压缩：数据缩减率可达5:1，节省存储空间
- 全局重删<br>
资源池内所有数据参与重删
- 前后台自适应<br>
基于负载的自适应，减少业务影响
- 多种介质<br>
支持全闪、混合介质

## 存储永新：0数据迁移，0业务中断

传统模式下，存储管理必须要经历： 设备替换+数据迁移
- 时间： 数月
- 高业务中断风险
- 成本： 15K$/TB

采用了软件定义存储之后： 软件滚动更新，数据自动均衡
- 时间： 数小时
- 0业务终端
- 无需专业服务

## 15分钟/TB高速数据重构，比传统存储恢复快20倍
SSD大Cache + 数据并行重建

分布式存储的 块存储中的每个硬盘都保存了多个数据块（Partition），这些数据块的副本按照策略分散在系统中的其他节点。当分布式存储的 块存储检测到硬盘或者节点硬件发生故障时，自动在后台启动数据修复。由于数据块的副本被分散到多个不同的存储节点上，数据修复时，将会在不同的节点上同时启动数据重建，每个节点上只需重建一小部分数据，多个节点并行工作，有效避免单个节点重建大量数据所产生的性能瓶颈，对上层业务的影响做到最小化。

工作流程：数据分片存储—>硬件故障—>故障自动检测—>自动重构副本—>多节点并行恢复。

分布式存储的 块存储支持并行、快速故障处理和重建：
- 数据块（Partition）及其副本分散在整个资源池内，硬盘故障后，可在资源池范围内自动并行重建。
- 数据分布上支持跨服务器，不会因某个服务器故障导致的数据不可访问和不可重建。
- 故障或者扩容时可以自动进行负载均衡，应用无需调整即可获得更大的容量和性能。

## 单桶1000亿文件存储，满足新兴业务海量小对象需求

比较常见的应用场景：
1. 票据影像
- 千亿级别对象，单套百亿级别文件
- 性能：TPS 1000~10000
2. 卡口图片
- 某大城市，2W路交通摄像头，全年图片数1400亿张
3. IoT车联网
- 大对象，带宽>250MB/s
- 小对象，单桶TPS 300~600

## 云上云下协同，全生命周期智能管理，提升管理效率

云上训练为主，本地训练为辅涵盖全生命周期，包含从资源规划，到业务发放，到系统调优，到风险预测，到故障定位；

云上训练： 充分利用2PB+的特征数据，支持1000+场景。

本地训练： 强调增强训练，提升个性化体验；

XX存储叠加AI的能力，采用了独特的云上训练结合本地增量训练，这样在云上利用数据量大的优势形成基线，同时在本地增量训练，结合具体业务提供个性化调优。

基于这种创新，实现智能管理，如资源的规划从被动到可提前60天预测容量，资源发放从复杂的手工操作到基于经验模板一键式发放，性能调优从手动到自学习自调优，故障从被动处理到可以提前14天预测出即将故障的硬盘，以及根据经验库更加快速准确故障定位等。

这样有什么好处呢？ 例如中航信是市场领先的航空运输旅游业信息技术提供商，拥有全球最大的票据结算系统（Billing and Settlement），2018年处理的民航客票达4亿张。在使用XX智能风险预测服务后，通过对运行系统中数千片硬盘状态的实时监测，提前14天预测出硬盘故障，把风险消除在萌芽之中，让业务更加可靠。

XX内部IT自己的数据中心内有超过500PB的数据。其中传统存储架构的100PB数据，需要5名运维人员。而部署分布式存储的400PB的数据，仅需3名运维人员，运维效率相比传统架构提升6倍 。

## 智能风险预测：提前洞悉存储性能、容量走势，简化管理，提升资源利用率

依托XX云AI深度学习，可以做到：
1. 性能容量趋势60天预判（分析潮汐规律，识别性能瓶颈）
2. 一键式资源发放，1000+ 应用模板
3. 个性化调优，100%满足SLA诉求
4. 提前14天发现故障盘
5. 2000+故障模式库，93%问题发现即给出方案


# 延伸阅读2

未来二三十年，人类一定会进入万物感知、万物互联和万物智能的智能社会。万物感知是入口、万物互联是基础、而万物智能是结果

后摩尔定律时代，技术快速发展，智能世界的核心技术正变得越来越清晰；5G、AI和视频三种技术相互促进、相互激发，加速智能世界的到来。视频+AI，当前49% AI行业应用以视频为基础，视频加速了AI在行业的落地；AI+5G，5G提供的大带宽和低时延让AI的算力无所不及；5G+视频，加速超高清视频（4K/8K）普及，催生了更多行业智能化场景

## 智能视频正成为行业智能化升级的关键，但是仍然面临居多挑战（这个观点值得怀疑）

众所周知，没有智能手机，就没有移动互联网。智能手机扮演着几个重要角色，它是用户体验的入口、也是应用的载体，更是人和人联接的纽带；视频在行业智能化中正扮演着类似的角色，人类的信息获得83%来源于视觉，机器视觉同样也是数字世界的眼睛，提供大量的数据输入；有了AI技术的加持，摄像机也正在成为入口、也是应用的最小载体，正成为物物联接、物理世界与智能世界联接的纽带；所以，智能视频就是行业智能化的“智能手机”！

千行百业的智能化有千百种场景，而当前主要基于垂直生态的智能视频解决方案，参与到千行百业的智能化转型仍然面临居多挑战。比如深圳某单位，有9个业务部门、31个业务系统、147个应用模块，各部门之间争相引入各种智能识别系统，最后造成各个系统形成一个个烟囱，各系统的数据也形成了一个个数据孤岛，阻碍了智能的深化；每出现一个新需求就要新建一个烟囱的系统，设备平台不支持，也无法找更多的生态伙伴提供更多的智能算法。另外一个例子是关于深圳新能源汽车车牌的识别，传统的算法都是固化在摄像机一起的，当新能源车牌识别需求出现后，需要人工上站一个一个去升级，3000路交通卡口升级了一个月才完成。通过这些案例都可以看到，封闭的平台和割裂的生态，是智能视频深入千行百业的核心挑战！


## 推动产业发展，重定义智能视频

重定义智能视频技术架构，让设备和算法解耦，构建开放式的平台。前端摄像机软件定义，按需加载，让通用摄像机秒变专用摄像机。后端视频云平台全云架构，多算法融合，可以打通数据孤岛，实现算力、算法、数据和任务协同。
传统摄像机是专机专用，特点是“算法固化+嵌入式系统+CPU通用处理器”。

而XX软件定义摄像机，从“功能机”到“智能机”，特点是：“软件+硬件的生态+开放的操作系统+NPU专业AI芯”。客户可以根据需求场景，灵活选择各种算法，实现各种智能应用。另外在XX下一代架构里面，也会讲硬件模块化设计，预留标准接口，接入其他的传感器，让末端实现真正的全息感知。

传统的视频云平台建设是堆叠模式，人脸是一套系统、车牌是一套系统，相互之间是割裂的；另外视频的接入、存储、解析、转发、检索等各项功能都需要一个盒子，而且只能加载集成过的算法；这样造成一个个数据孤岛，数据处理难以拉通，数据智能不能充分释放。而XX的智能视频云平台，完全基于云化架构设计，最新的vPaaS 2.0，支持通过SDK和API方式加载各种算法，可以实现一周上车，实现多算法直接的协调配合，同时支持多种算法场景，满足各行业各细分场景的智能需求。

智能深入千行百业，算法需求的长尾化个性化效应越来越明显，而当前的点对点、项目型交易、线下集成的方式，类似于智能手机时代的塞班系统时代，没有一个开放的“交易市场”，让使用者和开发者无法有效沟通，压抑了智能化需求。

为了让使用者按需可选，经济使用；让开发者专注需求，快速迭代。XX发布最新**智能视频新生态的算法商城**。基于XX开放的端边云平台，通过HoloSens商城专业的交易平台、完备的开发平台，**重新定义了智能视频的生态模式**。该商城会提供质量严选服务，让使用者随意挑，快速换，放心用，另外针对开发者还提供“**三云（训练云、调试云、开发云）一社区（开发者社区）**”，让开发者便捷上线，商业成功。

阅读理解：

1. 重新定了新的视频生态。

这个很有新意，传统的视频生态是什么样子的？引入了5G和AI之后视频的生态是什么样子的？

2. 摄像头这个领域

从“功能机”到“智能机”，进入了软硬一体的时代，周边配套的智能芯片，软件开发等生态逐步发展起来了；


波分当前有三板斧，成为世界第一；
存储的三板斧在哪？
海外高端市场；
