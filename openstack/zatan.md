# 6. 杂谈

## 6.1 存储DWPD
摘要：SSD发展的历史，从EMLC到CLMC、到TLC、未来到QLC，DWPD值越来越低，但是存储系统的可靠性和使用年限却越来越好。为什么？ DWPD的英文名称是Diskful Writes Per Day（每日满容量写入次数），SSD介质的可写次数已经不是存储可靠性的关键，可靠性的关键是软件对SSD物理空间的写入均衡的算法。一个均衡的写入算法，可以保障SSD容量平均磨损。假设10块600GB SSD，每日平均一次均衡写入，数据量是6TB，那么5年维保期间总写入数据量是6*365*

SSD发展的历史，从EMLC到CLMC、到TLC、未来到QLC，DWPD值越来越低，但是存储系统的可靠性和使用年限却越来越好。为什么？

DWPD的英文名称是Diskful Writes Per Day（每日满容量写入次数），SSD介质的可写次数已经不是存储可靠性的关键，可靠性的关键是软件对SSD物理空间的写入均衡的算法。一个均衡的写入算法，可以保障SSD容量平均磨损。假设10块600GB SSD，每日平均一次均衡写入，数据量是6TB，那么5年维保期间总写入数据量是6*365*5 = 10.95PB。企业极少具备10PB数据量，而只购买一个6TB容量的存储设备。业界有大量的研究论文已经论证，DWPD的实用价值远不如存储阵列算法带来的稳定性可信，其中比较著名的论文是2015年google在自己的数据中心做的关于SSD阵列磨损度统计的报告。
华为主流发货的SSD的DWPD为1， 业界主流友商的SSD DWPD一般为1和3，少量友商具备DWPD=10的SSD。高DWPD的SSD主要的应用场景是单盘频繁读写的存储或者计算设备，总体上，领先的存储企业已经规模应用TLC，且很快会走向规模应用QLC。

**一些优化磨损的算法创新是有效提供延长SSD盘数据的手段。**

解释 DWPD，TBW，颗粒原始PE cycle的关系：

估计业务需要的每天写入数据量：Drive Writes Per Day (DWPD).  Terabytes Written (TBW)

同时下面文章解释 DWPD ，TBW 实际上是一回事， DWPD说的一个盘每天写入的数据量/SSD盘容量（例如400GB 容量的SSD盘， 0.3 DWPD的寿命规格，表示 5年维保生命期内，每天可以写入 0.3X400=120GB数据）。 TBW = 每天写的数据量 X 5 X 365（转换为TB 单位）.

很多人认为颗粒原始的 PE cycle 次数就是，写入的SSD盘SAS/SATA接口上写入的数据次数。 这个理解有些误解。 这中间有转换参数（与业务模型相关），不能直接等同。   因为计算相对依赖业务IO特点，所以DWPD规格通常是以最严酷IO模型进行宣称的。（4K 全随机写的IO模型）。 如果IO 模型不是这么残酷，则支持5年的DWPD实际值更大，也就是每天可以写入更多数据，仍然满足寿命要求。

所以估计阵列中每个SSD 每天写入的数据量，是评估应用对SSD寿命需求的方法。

同时，根据客户应用的分析，配套SSD满足95%以上应用，仅写入量特别大的不同应用需要考虑分析该值。

## 6.2 Openstack容器化

Redhat自己搞了容器化的方案，但是已经放弃贡献社区了：
1. 社区推动力不足，代码在社区得不到价值提升；
2. 该领域过于专业，每个厂商基于自己的研究搞。


### 6.2.1 MariaDB的高可用主备数据库容器实践

设计的想法是：
1. 追求多容器实例，主备部署
2. 次之支持数据库定期备份，恢复
3. 为了部署的简单，不考虑使用swarm或者k8s进行容器的调度和编排。
4. 测试调通后，交给自研调度部署软件处理；


网上有一个实践：

https://severalnines.com/database-blog/running-mariadb-galera-cluster-without-orchestration-tools-db-container-management

这个也不错，支持读写分离，但是涉及其他管理器，生产可以考虑；
https://github.com/erasys/mariadb-ha-test
